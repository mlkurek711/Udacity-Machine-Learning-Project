1.  The goal of this project was to use machine learning techniques to identify persons of interest (poi) in the Enron dataset based on different features.  The set of data and emails is rather large which makes it difficult to sift through and analyze.  Using maching learning techniques we can create algorithms to help identify features that will help with the poi identification.  
There were 146 total data points orginally, but after removing a couple outliers 144 remained.  The data points 'TOTAL' and 'THE TRAVEL AGENCY IN THE PARK' were removed as they were rows in the spreadsheet and not actual names.  There are 18 people that are persons of interest (poi) which is 12.5% of the cleaned data.  There are 20 features in the original feature list, of which the emails to and from pois and bonus and salary are most interesting in terms of determining other pois.  More interactions with pois and higher bonus payouts as compared to a percentage of the salary would be red flags to me.

2. I created the feature bonus_to_salary ratio.  The rationale being the higher the ratio the more likely the person is a poi.  It did not make the top five features from the SelectKBest algorithm (it came in seventh with a score of 10.784).  I also made to_poi_message_ratio and from_poi_message_ratio thinking the more a person interacted with a poi the more likely they are a poi themself.  The to_poi_message_ratio made the top five with a score of 16.41.

3. I used a Decision Tree Clasifier which gave me an accuracy of 0.8604651162790697.  I tried Naive Bayes which gave a terrible accuracy of 0.3953488372093023 and a precision of 0.19118.  The recall for my Naive Bayes was very high at 0.78450 so it does not give very many false negatives.  My SVM had a good accuracy at 0.8837209302325582, but I could never get it to classify any true positives.

4. Tuning the parameters of an algorithm gives the machine learning algorithm confines and thresholds in which to classify the data.  The better the parameters are set, the more accurate and more efficient the machine learning algorithm.
I use a minimum sample split of 2 in my Decision Tree Clasifier.  Increasing the min_samples_split significantly gave a slightly higher accuracy, the precision and recall tanked.  Not splitting the data enough returned too many false negatives and false positives.

5. Validation ensures that the machine learning algorithm classifies the data well so you can analyze the most accurate data.  Overfitting is a common mistake in machine learning where the algorithm tries too hard to fit the data so although it does well with training data it loses it's effectiveness in the test data.  The project uses train/test split for validation on the data set.

6.  My Decision Tree Classifier gave me an accuracy of 0.8604651162790697 with a minimum sample split of 2 (with a min_sample_split of 100 the accuracy improved to 0.8837209302325582 but the precion and recall tanked as discussed in the answer to question 3).  My precision was consistently over .305, but my recall would dip slightly below .3 (.2995, .2975) at times.  It does often return values above .3.  My classifier must return more false positives than false negatives to have that kind of inconsistency in my recall.

